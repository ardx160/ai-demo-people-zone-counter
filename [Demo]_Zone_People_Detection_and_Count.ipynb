{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ardx160/ai-demo-people-zone-counter/blob/main/%5BDemo%5D_Zone_People_Detection_and_Count.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hello and welcome..\n",
        "\n",
        "..to this awesome Google Colab workshop, where you will learn how to do some cool things with code and AI. Before we start, here are some things that you need to prepare:\n",
        "\n",
        "### 1. You need to have a Google account to run the colab. If you donâ€™t have one, you can create one for free here.\n",
        "\n",
        "### 2. You need to open this bit.ly link: `bit.ly/ahmdemoai`. This will take you to the colab notebook that we will use for the workshop.\n",
        "\n",
        "### 3. You can also bookmark this link for future reference. You need to make a copy of the colab notebook in your Google Drive. To do this, click on the File menu, then select Save a copy in Drive. This will allow you to edit and run the code in the notebook.\n",
        "\n",
        "### 4. You need to have a good internet connection and a web browser that supports Google Colab. You can check the system requirements here.\n",
        "\n",
        "\n",
        "Thatâ€™s all you need to prepare for the workshop. If you have any questions or issues, please let me know. Iâ€™m here to help you along the way. I hope you are excited and ready to have some fun with code and AI. ðŸ˜Š\n",
        "\n"
      ],
      "metadata": {
        "id": "bWbnaeGXJIrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. Download and Import Libraries\n",
        "\n",
        "*   Welcome to this Google Colab notebook, where you will learn how to do cool things with code. In this cell, you will download and import the libraries that you need. These libraries will help you with tasks like deep learning, computer vision, image processing, web applications, and more. You will also download and compress a sample video file. You will then see the video in the output.\n",
        "\n",
        "*   To run the cell, you can click on the play button on the left side of the cell. You will see a spinning icon while the cell is running, and a check mark when it is done. You will also see the output of the cell below the code. If you see any errors or warnings in red color, you need to fix them.\n",
        "\n",
        "*   This cell might take some time to run, so please be patient. You can use this time to relax, chat, or read the code comments. Donâ€™t close this tab or refresh the page, or you will lose your progress. Iâ€™m here to help you along the way. ðŸ˜Š\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l343ULO6A4oK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0EL4o_eOQmej"
      },
      "outputs": [],
      "source": [
        "# @title #### `Download and Import Libraries: Click play button to run this cell`\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "from shapely.geometry import Polygon\n",
        "from shapely import affinity\n",
        "import cv2\n",
        "import datetime\n",
        "from base64 import b64encode\n",
        "import plotly.graph_objects as go\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# Define the path for the temp folder\n",
        "temp_path = '/tmp'\n",
        "\n",
        "# Define the initial polygon coordinates\n",
        "polygon = np.array([\n",
        "    [540,  985 ],\n",
        "    [1620, 985 ],\n",
        "    [1620, 2065],\n",
        "    [540,  2065]\n",
        "], np.int32)\n",
        "\n",
        "# Clone the yolov5 repository to the temp folder\n",
        "%cd {temp_path}\n",
        "!git clone -q https://github.com/ultralytics/yolov5 # Add -q flag to suppress output\n",
        "\n",
        "# Install the required packages\n",
        "!pip install -q -r yolov5/requirements.txt # Add -q flag to suppress output\n",
        "!pip install -q supervision==0.2.0 # Add -q flag to suppress output\n",
        "!pip install -q dash # Add -q flag to suppress output and remove comment\n",
        "\n",
        "# Import dash and supervision\n",
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output, State\n",
        "import supervision as sv\n",
        "# print(\"supervision\", sv.__version__) # Comment out this line to hide the supervision version\n",
        "\n",
        "# Download the video file to the temp folder\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr\" -O market-square.mp4 && rm -rf /tmp/cookies.txt # Add -q flag to suppress output\n",
        "MARKET_SQUARE_VIDEO_PATH = os.path.join(temp_path, 'market-square.mp4')\n",
        "\n",
        "# Load the yolov5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')\n",
        "\n",
        "# Compress the video and save it to the compressed path\n",
        "crf = 28\n",
        "compressed_path = \"/content/market-square-result-c.mp4\"\n",
        "# Use the check_call function to execute the ffmpeg command\n",
        "subprocess.check_call(f\"ffmpeg -i {MARKET_SQUARE_VIDEO_PATH} -vcodec libx264 -crf {crf} -vf scale=640:-2 {compressed_path}\", shell=True)\n",
        "# Collect the garbage to free up memory\n",
        "gc.collect()\n",
        "\n",
        "# Clear the output of the cell\n",
        "display.clear_output()\n",
        "\n",
        "# Show the video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"<video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "       </video>\"\"\" % data_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. AI Detection Capability\n",
        "\n",
        "*   In this cell, you will see how AI can detect people in a crowded scene from one frame of a video. You will use the yolov5 model to detect and draw boxes around all the people in the frame, and show some information about each person. You will then display the annotated frame in the output. To run the cell, click on the play button on the left side of the cell, or press `Shift + Enter` on your keyboard. If you see any errors or warnings in red color, you need to fix them. You will be amazed by how many people the AI can detect in one frame. You can zoom in and out of the frame, count the people, find yourself or your friends, or admire the scene and the AI. Have fun with it, and donâ€™t worry, Iâ€™m here to help you. ðŸ˜Š\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PfdSDvYUrA6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u4Bp6QAQsIN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title #### `AI Detection Capability: Click play button to run this cell`\n",
        "\n",
        "# extract video frame\n",
        "generator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\n",
        "iterator = iter(generator)\n",
        "frame = next(iterator)\n",
        "\n",
        "# detect\n",
        "results = model(frame, size=1280)\n",
        "detections = sv.Detections.from_yolov5(results)\n",
        "\n",
        "# annotate\n",
        "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
        "frame = box_annotator.annotate(scene=frame, detections=detections)\n",
        "\n",
        "%matplotlib inline\n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Zone-People AI Tracking\n",
        "*   In this cell, you will process the whole video file and see how AI can detect people in different frames. You will use the polygons and zones that you defined or created, and the same yolov5 model. You will also set a minimum detection threshold, and see how the AI can track the time that each polygon has people in it. You will then compress and display the video in the output.\n",
        "\n",
        "*   To run the cell, you can use the same methods as before. You will see a spinning icon, a check mark, and the output. If you see any errors or warnings, you need to fix them. You will see how the AI can detect and track people in the video. You can also change the minimum detection threshold, and compare the original and the compressed video. You can also run this cell again after creating a new polygon in cell D, and see how it affects the results. Have fun with it, and donâ€™t worry, Iâ€™m here to help you. ðŸ˜Š\n",
        "\n"
      ],
      "metadata": {
        "id": "RcvVlklRuqa3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R85unxdjlInC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title #### `Zone-People AI Tracking: Click play button to run this cell`\n",
        "\n",
        "save_path = \"/content/market-square-result.mp4\"\n",
        "\n",
        "if Path(save_path).exists():\n",
        "  os.remove(save_path)\n",
        "if Path(compressed_path).exists():\n",
        "  os.remove(compressed_path)\n",
        "\n",
        "# Define a function to get the time text from the frame index and the frame rate\n",
        "def get_time_text(frame_index, frame_rate):\n",
        "  elapsed_time = frame_index / frame_rate\n",
        "  # Change the format to minutes and seconds\n",
        "  time_text = f\"{int(elapsed_time // 60):02d}:{int(elapsed_time % 60):02d}\"\n",
        "  return time_text\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "color = (255, 255, 255)\n",
        "rect_width = 200\n",
        "rect_height = 80\n",
        "margin = 10\n",
        "\n",
        "polygons = [polygon]\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n",
        "\n",
        "colors = sv.ColorPalette.default()\n",
        "thickness = 4\n",
        "text_scale = 2\n",
        "zones = []\n",
        "zone_annotators = []\n",
        "box_annotators = []\n",
        "for index, polygon in enumerate(polygons):\n",
        "  zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n",
        "  zones.append(zone)\n",
        "  zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=colors.by_idx(index), thickness=thickness*2, text_thickness=thickness*2, text_scale=text_scale*2)\n",
        "  zone_annotators.append(zone_annotator)\n",
        "  box_annotator = sv.BoxAnnotator(color=colors.by_idx(index), thickness=thickness, text_thickness=thickness, text_scale=text_scale)\n",
        "  box_annotators.append(box_annotator)\n",
        "\n",
        "# Create a list of frame indices for each polygon\n",
        "polygon_frame_indices = [0] * len(polygons)\n",
        "\n",
        "# Define a function to process each frame of the video, using the yolov5 model and the supervision library\n",
        "def process_frame(frame: np.ndarray, i) -> np.ndarray:\n",
        "  results = model(frame, size=1280)\n",
        "  detections = sv.Detections.from_yolov5(results)\n",
        "  detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n",
        "\n",
        "  # Loop through the zones, zone annotators, and box annotators\n",
        "  for index, (zone, zone_annotator, box_annotator) in enumerate(zip(zones, zone_annotators, box_annotators)):\n",
        "    mask = zone.trigger(detections=detections)\n",
        "    detections_filtered = detections[mask]\n",
        "    frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n",
        "    frame = zone_annotator.annotate(scene=frame)\n",
        "\n",
        "    time_text = get_time_text(polygon_frame_indices[index], video_info.fps)\n",
        "    text_size, _ = cv2.getTextSize(time_text, font, text_scale, thickness)\n",
        "    x = int(polygons[index][0][0])\n",
        "    y = int(polygons[index][0][1])\n",
        "    x1 = x - margin\n",
        "    y1 = y - margin\n",
        "    x2 = x + text_size[0] + margin\n",
        "    y2 = y + text_size[1] + margin\n",
        "\n",
        "    frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), -1)\n",
        "    frame = cv2.putText(frame, time_text, (x, y + text_size[1]), font, text_scale, color, thickness, cv2.LINE_AA)\n",
        "    detection_count = len(detections_filtered)\n",
        "    if detection_count >= min_detection:\n",
        "      polygon_frame_indices[index] = i\n",
        "  return frame\n",
        "\n",
        "min_detection = 9 #@param {type:\"integer\"}\n",
        "\n",
        "sv.process_video(source_path=MARKET_SQUARE_VIDEO_PATH, target_path=save_path, callback=process_frame)\n",
        "\n",
        "crf = 28\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 -crf {crf} -vf scale=640:-2 {compressed_path}\")\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "display.HTML(\"\"\"<video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "       </video>\"\"\" % data_url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Zone Editor App\n",
        "*   In this cell, you will create your own polygon zone on the video frame. You will use a dash app to draw a new rectangle on the figure and save it as a polygon. You will then see the coordinates of the new polygon in the output. You can use this polygon to define a new zone for the AI detection.\n",
        "\n",
        "*   To run the cell, you only need to do it once. You will see a spinning icon, a check mark, and the output. If you see any errors or warnings, you need to fix them. You will see a dash app with a figure and a button. You can draw a new rectangle on the figure by clicking and dragging on it. You can also erase the existing rectangle by clicking on the eraser icon. You can then click on the button to save the new polygon and see its coordinates. You can repeat this process as many times as you want, without running the cell again. Have fun with it, and donâ€™t worry, Iâ€™m here to help you. ðŸ˜Š"
      ],
      "metadata": {
        "id": "v7U2fBS_w8v-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qGoJe-wAM8tC"
      },
      "outputs": [],
      "source": [
        "# @title #### `Zone Editor App: Click play button to run this cell`\n",
        "\n",
        "\n",
        "# Get a frame from the sample video and convert it to a PIL Image object\n",
        "video = imageio.get_reader(MARKET_SQUARE_VIDEO_PATH, \"ffmpeg\")\n",
        "frame = video.get_data(0)\n",
        "frame = Image.fromarray(frame)\n",
        "\n",
        "# Create a figure with the frame as background\n",
        "fig = go.Figure()\n",
        "fig.add_layout_image(\n",
        "    dict(\n",
        "        source=frame, # Pass the PIL Image object\n",
        "        xref=\"x\",\n",
        "        yref=\"y\",\n",
        "        x=0,\n",
        "        y=0, # Change the y position to 0\n",
        "        sizex=video.get_meta_data()[\"size\"][0], # Change the sizex to match the video width\n",
        "        sizey=video.get_meta_data()[\"size\"][1], # Change the sizey to match the video height\n",
        "        sizing=\"stretch\",\n",
        "        layer=\"below\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Update the axes and layout\n",
        "fig.update_xaxes(\n",
        "    showgrid=False,\n",
        "    zeroline=False,\n",
        "    range=[0, video.get_meta_data()[\"size\"][0]]\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    showgrid=False,\n",
        "    zeroline=False,\n",
        "    range=[video.get_meta_data()[\"size\"][1], 0], # Reverse the y-axis range\n",
        "    scaleanchor=\"x\",\n",
        "    scaleratio=1\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Polygon Editor\",\n",
        "    dragmode=\"drawrect\", # Allow drawing new rectangles\n",
        "    hovermode=False,\n",
        "    newshape=dict(line_color=\"cyan\"),\n",
        "    activeshape=dict(opacity=0.8) # Remove line_color property\n",
        ")\n",
        "\n",
        "# Create a dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Define the app layout\n",
        "app.layout = html.Div([\n",
        "    html.H3(\"Draw a new rectangle on the figure and click the button to save it\"),\n",
        "    dcc.Graph(id=\"graph\", figure=fig, config={'modeBarButtonsToAdd':['drawrect','eraseshape']}), # Add the config argument here\n",
        "    html.Button(\"Save new polygon\", id=\"button\"), # Add a button to save the new polygon\n",
        "    html.Div(id=\"output\") # Add a div to display the output\n",
        "])\n",
        "\n",
        "# Define a dash callback\n",
        "@app.callback(\n",
        "    Output(\"output\", \"children\"), # The output is the children of the output div\n",
        "    Input(\"button\", \"n_clicks\"), # The input is the number of clicks of the button\n",
        "    State(\"graph\", \"relayoutData\") # The state is the relayout data of the graph\n",
        ")\n",
        "def save_new_polygon(n_clicks, relayoutData):\n",
        "    # Check if the button is clicked and the relayout data is not empty\n",
        "    if n_clicks and relayoutData:\n",
        "        # Get the x and y coordinates of the new rectangle\n",
        "        x0 = relayoutData[\"shapes[0].x0\"]\n",
        "        y0 = relayoutData[\"shapes[0].y0\"]\n",
        "        x1 = relayoutData[\"shapes[0].x1\"]\n",
        "        y1 = relayoutData[\"shapes[0].y1\"]\n",
        "        # Create a new polygon array from the rectangle coordinates\n",
        "        new_polygon = np.array([\n",
        "            [x0, y0],\n",
        "            [x1, y0],\n",
        "            [x1, y1],\n",
        "            [x0, y1]\n",
        "        ], np.int32)\n",
        "\n",
        "        global polygon\n",
        "        polygon = new_polygon\n",
        "        # Return the new polygon coordinates as the output\n",
        "        return f\"New polygon coordinates: {polygon}\"\n",
        "    # If the button is not clicked or the relayout data is empty, return an empty output\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "# Run the app\n",
        "app.run_server(mode=\"inline\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}